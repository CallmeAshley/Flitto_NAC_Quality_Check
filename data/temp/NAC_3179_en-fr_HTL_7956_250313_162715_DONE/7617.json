{
    "source": "en_US",
    "target": "fr_FR",
    "text": "Abstract:\nIn recent years, leveraging the powerful contextual understanding and extensive general knowledge of Large Language Models (LLMs) to enhance Temporal Knowledge Graphs (TKGs) Completion (TKGC) has emerged as a natural progression in research. However, the complexity of TKGs often prevents the generation of a Comprehensive and Concise Knowledge Sequence (CCKS) from historical knowledge through manually defined rules, which is essential for LLMs to infer future knowledge. To address this, we propose the Knowledge Attention via Radial Basis Functions (KA-RBF) model. Firstly, KA-RBF generates Original Knowledge Sequences (OKS) based on a simple manually defined rule. Building on this, KA-RBF uses the knowledge in the original training set along with the corresponding OKS of the head and tail entities as training samples, optimizing to enhance the attention on similar knowledge pairs. Next, KA-RBF scores the knowledge at each timestamp in the OKS of the target prediction sample to filter knowledge and generate the CCKS. Finally, we conducted three different control experiments and validated the effectiveness of KA-RBF across four different datasets. Additionally, we performed multi-faceted data analysis experiments on the existing four benchmark datasets, further reinforcing the feasibility of KA-RBF while also highlighting future research directions for TKGC.\nLast Modified: Wed, 09 Oct 2024 08:04:22 GMT\nCreated on: Wed, 09 Oct 2024 08:04:22 GMT",
    "trans": "Résumé :\nCes dernières années, l’exploitation des puissantes capacités de compréhension contextuelle et de la vaste connaissance générale des grands modèles de langage (LLMs) pour améliorer l’accomplissement des Graphes de Connaissances Temporels (TKGC) est devenue une évolution naturelle dans la recherche. Cependant, la complexité des TKGs empêche souvent la génération d’une Séquence de Connaissances Complète et Concise (CCKS) à partir des connaissances historiques via des règles définies manuellement, ce qui est essentiel pour que les LLMs puissent inférer des connaissances futures. Pour y remédier, nous proposons le modèle Knowledge Attention via Radial Basis Functions (KA-RBF). Tout d’abord, KA-RBF génère des Séquences de Connaissances Originales (OKS) basées sur une règle manuelle simple. Ensuite, KA-RBF utilise les connaissances du jeu de données d’entraînement ainsi que les OKS des entités tête et queue comme échantillons d’entraînement, optimisant ainsi l’attention sur les paires de connaissances similaires. KA-RBF évalue ensuite les connaissances à chaque instant temporel dans l’OKS de l’échantillon cible pour filtrer les connaissances et générer la CCKS. Enfin, nous avons mené trois expériences de contrôle distinctes et validé l’efficacité de KA-RBF sur quatre ensembles de données différents. De plus, nous avons effectué des analyses approfondies sur ces quatre ensembles de données de référence, renforçant ainsi la pertinence de KA-RBF tout en mettant en évidence de nouvelles pistes de recherche pour le TKGC.\nDate de dernière modification : Mercredi 9 octobre 2024, 08:04:22 GMT\nDate de création : Mercredi 9 octobre 2024, 08:04:22 GMT"
}