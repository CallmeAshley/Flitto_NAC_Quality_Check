{
    "source": "en_US",
    "target": "fr_FR",
    "text": "If you've followed my work on RAG systems, you'll know I emphasize treating them as recommendation systems at their core. In this post, we'll explore the concept of inverted thinking to tackle the challenge of building an exceptional RAG system.\nWhat is inverted thinking?\nInverted thinking is a problem-solving approach that flips the perspective. Instead of asking, \"How can I build a great RAG system?\", we ask, \"How could I create the worst possible RAG system?\" By identifying potential pitfalls, we can more effectively avoid them and build towards excellence.\nThis approach aligns with our broader discussion on RAG systems, which you can explore further in our RAG flywheel article and our comprehensive guide on Levels of Complexity in RAG Applications.\nInventory\nYou'll often see me use the term inventory. I use it to refer to the set of documents that we're searching over. It's a term that I picked up from the e-commerce world. It's a great term because it's a lot more general than the term corpus. It's also a lot more specific than the term collection. It's a term that can be used to refer to the set of documents that we're searching over, the set of products that we're selling, or the set of items that we're recommending.\nDon't worry about latency¶\nThere must be a reason that chat GPT tries to stream text out. Instead, we should only show the results once the entire response is completed. Many e-commerce websites have found that 100 ms improvement in latency can increase revenue by 1%. Check out How One Second Could Cost Amazon $1.6 Billion In Sales.\nDon't show intermediate results¶\nUsers love love staring at a blank screen. It's a great way to build anticipation. If we communicated intermittent steps like the ones listed below, we'd just be giving away the secret sauce and users prefer to be left in the dark about what's going on.\nUnderstanding your question\nSearching with \"...\"\nFinding the answer\nGenerating response\nDon't Show Them the Source Document¶\nNever show the source documents, and never highlight the origin of the text used to generate the response. Users should never have to fact-check our sources or verify the accuracy of the response. We should assume that they trust us and that there is no risk of false statements.\nWe Should Not Worry About Churn¶\nWe are not building a platform; we are just developing a machine learning system to gather metrics. Instead of focusing on churn, we should concentrate on the local metrics of our machine learning system like AUC and focus on benchmarks on HuggingFace.\nWe Should Use a Generic Search Index¶\nRather than asking users or trying to understand the types of queries they make, we should stick with a generic search and not allow users to generate more specific queries. There is no reason for Amazon to enable filtering by stars, price, or brand. It would be a waste of time! Google should not separate queries into web, images, maps, shopping, news, videos, books, and flights. There should be a single search bar, and we should assume that users will find what they're looking for.\nWe Should Not Develop Custom UI¶\nIt doesn't make sense to build a specific weather widget when the user asks for weather information. Instead, we should display the most relevant information. Semantic search is flawless and can effectively handle location or time-based queries. It can also re-rank the results to ensure relevance.\nWe Should Not Fine-Tune Our Embeddings¶\nA company like Netflix should have a generic movie embedding that can be used to recommend movies to people. There's no need to rely on individual preferences (likes or dislikes) to improve the user or movie embeddings. Generic embeddings that perform well on benchmarks are sufficient for building a product.\nWe Should Train an LLM¶\nRunning inference on a large language model locally, which scales well, is cost-effective and efficient. There's no reason to depend on OpenAI for this task. Instead, we should consider hiring someone and paying them $250k a year to figure out scaling and running inference on a large language model. OpenAI does not offer any additional convenience or ease of use. By doing this, we can save money on labor costs.\nWe Should Not Manually Curate Our Inventory¶\nThere's no need for manual curation of our inventory. Instead, we can use a generic search index and assume that the documents we have are relevant to the user's query. Netflix should not have to manually curate the movies they offer or add additional metadata like actors and actresses to determine which thumbnails to show for improving click rates. The content ingested on day one is sufficient to create a great recommendation system.\nWe Should Not Analyze Inbound Queries¶\nAnalyzing the best and worst performing queries over time or understanding how different user cohorts ask questions will not provide any valuable insights. Looking at the data itself will not help us generate new ideas to improve specific segments of our recommendation system. Instead, we should focus on improving the recommendation system as a whole and avoid specialization.\nImagine if Netflix observed that people were searching for \"movies with Will Smith\" and decided to add a feature that allows users to search for movies with Will Smith. That would be a waste of time. There's no need to analyze the data and make system improvements based on such observations.\nMachine Learning Engineers Should Not Be Involved in Ingestion¶\nMachine Learning Engineers (MLEs) do not gain valuable insights by examining the data source or consulting domain experts. Their role should be limited to working with the given features. Theres no way that MLEs who love music would do a better job at Spotify, or a MLE who loves movies would do a better job at Netflix. Their only job is to take in data and make predictions.\nWe Should Use a Knowledge Graph¶\nOur problem is so unique that it cannot be handled by a search index and a relational database. It is unnecessary to perform 1-2 left joins to answer a single question. Instead, considering the trending popularity of knowledge graphs on Twitter, it might be worth exploring the use of a knowledge graph for our specific case.\nWe should treat all inbound inventory the same¶\nThere's no need to understand the different types of documents that we're ingesting. How different could marketing content, construction documents, and energy bills be? Just because some have images, some have tables, and some have text doesn't mean we should treat them differently. It's all text, and so an LLM should just be able to handle it.\nWe should not have to build special ingestion pipelines¶\nGPT-4 has solve all of data processing so if i handle a photo album, a pdf, and a word doc, it should be able to handle any type of document. There's no need to build special injestion pipelines for different types of documents. We should just assume that the LLM will be able to handle it. I shouldn't dont even have to think about what kinds of questions I need to answer. I should just be able to ask it anything and it should be able to answer it.\nWe should never have to ask the data provider for clean data¶\nIf Universal studios gave Netflix a bunch of MOV files with no metadata, Netflix should not have to ask Universal studios to provide additional movie metadata. Universal might not know the runtime, or the cast list and its netflix's job to figure that out. Universal should not have to provide any additional information about the movies they're providing.\nWe should never have to cluster our inventory¶\nTheres only one kind of inventory and one kind of question. We should just assume that the LLM will be able to handle it. I shouldn't dont even have to think about what kinds of questions I need to answer. Topic clustering would only show us how uniform our inventory is and how little variation there is in the types of questions that users ask.\nWe should focus on local evals and not A/B tests¶\nOnce we run our GPT-4 self critique evaluations we'll know how well our system is doing and it'll make us more money, We should spend most of our time writing evaluation prompts and measuring precision / recall and just launching the best one. A/B tests are a waste of time and we should just assume that the best performing prompt will be the best performing business outcome.",
    "trans": "Si vous avez suivi mes travaux sur les systèmes RAG, vous savez que j'insiste sur le fait qu'il s'agit essentiellement de systèmes de recommandation. Dans ce billet, nous allons explorer le concept de la pensée inversée pour relever le défi de la construction d'un système RAG exceptionnel.\nQu'est-ce que la pensée inversée ?\nLa pensée inversée est une approche de résolution de problèmes qui inverse la perspective. Au lieu de demander « Comment puis-je construire un excellent système RAG ? », nous demandons « Comment pourrais-je créer le pire système RAG possible ? » En identifiant les écueils potentiels, nous pouvons les éviter plus efficacement et viser l'excellence.\nCette approche s'inscrit dans le cadre de notre discussion plus générale sur les systèmes RAG, que vous pouvez approfondir dans notre article sur le volant RAG et dans notre guide complet sur les niveaux de complexité dans les applications RAG.\nInventaire\nVous me verrez souvent utiliser le terme inventaire. Je l'utilise pour désigner l'ensemble des documents sur lesquels nous effectuons une recherche. C'est un terme que j'ai emprunté au monde du commerce électronique. C'est un excellent terme car il est beaucoup plus général que le terme corpus. Il est également plus spécifique que le terme collection. Il peut être utilisé pour désigner l'ensemble des documents que nous recherchons, l'ensemble des produits que nous vendons ou l'ensemble des articles que nous recommandons.\nNe vous inquiétez pas de la latence¶\nIl doit y avoir une raison pour que le chat GPT essaie de diffuser du texte en continu. Nous devrions plutôt n'afficher les résultats qu'une fois la réponse complète terminée. De nombreux sites de commerce électronique ont constaté qu'une amélioration de 100 ms de la latence peut augmenter les revenus de 1 %. Voir Comment une seconde peut coûter 1,6 milliard de dollars de chiffre d'affaires à Amazon.\nNe montrez pas les résultats intermédiaires¶\nLes utilisateurs adorent regarder un écran vide. C'est un excellent moyen de créer de l'anticipation. Si nous communiquions les étapes intermédiaires comme celles listées ci-dessous, nous divulguerions simplement la recette secrète et les utilisateurs préfèrent qu'on les laisse dans l'ignorance de ce qui se passe.\nCompréhension de votre question\nRechercher avec « ... »\nTrouver la réponse\nGénérer une réponse\nNe leur montrez pas le document source¶\nNe montrez jamais les documents sources et ne mettez jamais en évidence l'origine du texte utilisé pour générer la réponse. Les utilisateurs ne devraient jamais avoir à vérifier nos sources ou à valider l'exactitude de la réponse. Nous devons supposer qu'ils nous font confiance et qu'il n'y a aucun risque de fausses déclarations.\nNous ne devrions pas nous préoccuper du taux de désabonnement¶\nNous ne construisons pas une plateforme ; nous développons simplement un système d'apprentissage automatique pour recueillir des métriques. Au lieu de nous concentrer sur lle taux de désabonnement, nous devrions nous concentrer sur les métriques locales de notre système d'apprentissage automatique, comme l'AUC, et sur les benchmarks sur HuggingFace.\nNous devrions utiliser un index de recherche générique¶\nAu lieu de demander aux utilisateurs ou de chercher à comprendre les types de requêtes qu'ils font, nous devrions nous en tenir à une recherche générique et ne pas permettre aux utilisateurs de générer des requêtes plus spécifiques. Il n'y a aucune raison pour qu'Amazon permette de filtrer par étoiles, prix ou marque. Ce serait une perte de temps ! Google ne devrait pas séparer les requêtes en web, images, cartes, shopping, actualités, vidéos, livres et vols. Il devrait y avoir une seule barre de recherche, et nous devons supposer que les utilisateurs trouveront ce qu'ils cherchent.\nNous ne devrions pas développer d'interface utilisateur personnalisée¶\nIl n'est pas judicieux de créer un widget météo spécifique lorsque l'utilisateur demande des informations sur la météo. Nous devrions plutôt afficher les informations les plus pertinentes. La recherche sémantique est sans faille et peut traiter efficacement les requêtes basées sur la localisation ou le temps. Elle peut également reclasser les résultats pour garantir leur pertinence.\nNous ne devrions pas affiner nos encodages¶\nUne entreprise comme Netflix devrait disposer d'un embedding générique de films pouvant être utilisé pour recommander des films aux gens. Il n'est pas nécessaire de se fier aux préférences individuelles (j'aime ou je n'aime pas) pour améliorer les embeddings des utilisateurs ou des films. Les embeddings génériques qui réussissent bien sur les benchmarks suffisent à la construction d'un produit.\nNous devrions entraîner un LLM¶\nExécuter l'inférence sur un grand modèle de langage localement, ce qui est évolutif, rentable et efficace. Il n'y a aucune raison de dépendre d'OpenAI pour cette tâche. Au lieu de cela, nous devrions envisager d'embaucher quelqu'un et de lui payer 250 000 $ par an pour comprendre la mise à l'échelle et l'exécution de l'inférence sur un grand modèle de langage. OpenAI n'offre pas de commodités ou de facilité d'utilisation supplémentaires. En faisant cela, nous pourrions économiser de l'argent sur les coûts de main-d'œuvre.\nNous ne devrions pas procéder à une curation manuelle de notre inventaire¶\nIl n'est pas nécessaire de procéder à une curation manuelle de notre inventaire. Nous pouvons utiliser un index de recherche générique et supposer que les documents dont nous disposons sont pertinents par rapport à la requête de l'utilisateur. Netflix ne devrait pas avoir à sélectionner manuellement les films qu'il propose ou à ajouter des métadonnées supplémentaires telles que les acteurs et les actrices pour déterminer quelles vignettes afficher afin d'améliorer les taux de clics. Le contenu ingéré le premier jour est suffisant pour créer un excellent système de recommandation.\nNous ne devrions pas analyser les requêtes entrantes¶\nAnalyser les meilleures et pires requêtes au fil du temps ou comprendre comment différents groupes d'utilisateurs posent des questions ne fournira aucune information précieuse. Regarder les données ne nous aidera pas à générer de nouvelles idées pour améliorer des segments spécifiques de notre système de recommandation. Au lieu de cela, nous devrions nous concentrer sur l'amélioration du système de recommandation dans son ensemble et éviter la spécialisation.\nImaginez que Netflix observe que les gens recherchent des « films avec Will Smith » et décide d'ajouter une fonction permettant aux utilisateurs de rechercher des films avec Will Smith. Ce serait une perte de temps. Il n'est pas nécessaire d'analyser les données et d'apporter des améliorations au système sur la base de telles observations.\nLes ingénieurs en apprentissage automatique ne devraient pas être impliqués dans l'ingestion¶\nLes ingénieurs en apprentissage automatique (MLE) n'obtiennent pas d'informations précieuses en examinant la source de données ou en consultant des experts du domaine. Leur rôle doit se limiter à travailler avec les fonctionnalités données. Il n'y a aucune chance que les MLE qui aiment la musique fassent un meilleur travail chez Spotify, ou qu'un MLE qui aime les films fasse un meilleur travail chez Netflix. Leur seule tâche consiste à prendre des données et à faire des prédictions.\nNous devrions utiliser un graphe de connaissances¶\nNotre problème est tellement unique qu'il ne peut pas être traité par un index de recherche et une base de données relationnelle. Il est inutile de réaliser des jointures pour répondre à une seule question. Au lieu de cela, étant donné la popularité croissante des graphes de connaissances sur Twitter, il pourrait être intéressant d'explorer l'utilisation d'un graphe de connaissances pour notre cas spécifique.\nNous devrions traiter tous les inventaires entrants de la même manière¶\nIl n'est pas nécessaire de comprendre les différents types de documents que nous ingérons. En quoi le contenu marketing, les documents de construction et les factures d'énergie peuvent-ils être différents ? Ce n'est pas parce que certains contiennent des images, d'autres des tableaux et d'autres encore du texte que nous devons les traiter différemment. Il ne s'agit que de texte, et un LLM devrait donc être capable de le traiter.\nNous ne devrions pas avoir à construire des pipelines d'ingestion spéciaux¶\nGPT-4 a résolu tout le traitement des données, donc si je gère un album photo, un PDF et un document Word, il devrait être capable de gérer n'importe quel type de document. Il n'est pas nécessaire de créer des pipelines d'ingestion spéciaux pour différents types de documents. Nous devons simplement supposer que le LLM pourra gérer cela. Je ne devrais même pas avoir à réfléchir aux types de questions auxquelles je dois répondre. Je devrais simplement pouvoir lui poser n'importe quelle question et il devrait pouvoir y répondre.\nNous ne devrions jamais demander au fournisseur de données des données propres¶\nSi Universal Studios donne à Netflix des fichiers MOV sans métadonnées, Netflix ne devrait pas avoir à demander à Universal Studios de fournir des métadonnées supplémentaires sur les films. Universal pourrait ne pas savoir la durée ou la liste des acteurs, et c'est à Netflix de le découvrir. Universal ne devrait pas avoir à fournir d'informations supplémentaires sur les films qu'ils fournissent.\nNous ne devrions jamais avoir à regrouper notre inventaire¶\nIl n'y a qu'un seul type d'inventaire et un seul type de question. Nous devrions juste supposer que le LLM pourra gérer cela. Je ne devrais même pas avoir à réfléchir aux types de questions auxquelles je dois répondre. Le regroupement par sujet ne ferait que montrer à quel point notre inventaire est uniforme et à quel point il y a peu de variation dans les types de questions que les utilisateurs posent.\nNous devrions nous concentrer sur les évaluations locales et non sur les tests A/B¶\nUne fois que nous aurons effectué nos évaluations d'autocritique GPT-4, nous saurons si notre système fonctionne bien et s'il nous rapporte plus d'argent. Nous devrions passer le plus clair de notre temps à rédiger des messages d'évaluation, à mesurer la précision et le rappel et à lancer le meilleur. Les tests A/B sont une perte de temps et nous devrions simplement supposer que l'incitation la plus performante sera le résultat commercial le plus performant."
}