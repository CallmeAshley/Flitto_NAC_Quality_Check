{
    "source": "en_US",
    "target": "pt_BR",
    "text": "In PPO, the goal is to find an improved policy for an agent by iteratively updating its parameters based on the rewards received from interacting with the environment. However, updating the policy too aggressively can lead to unstable learning or drastic policy changes. To address this, PPO introduces a constraint that limits the extent of policy updates. This constraint is enforced by using KL-Divergence.\nTo understand how KL-Divergence works, imagine we have two probability distributions: the distribution of the original LLM, and a new proposed distribution of an RL-updated LLM. KL-Divergence measures the average amount of information gained when we use the original policy to encode samples from the new proposed policy. By minimizing the KL-Divergence between the two distributions, PPO ensures that the updated policy stays close to the original policy, preventing drastic changes that may negatively impact the learning process.\nA library that you can use to train transformer language models with reinforcement learning, using techniques such as PPO, is TRL (Transformer Reinforcement Learning). In",
    "trans": "Na PPO, o objetivo é encontrar uma política melhorada para um agente ao atualizar de forma iterativamente seus parâmetros com base nas recompensas recebidas por meio da interação com o ambiente. No entanto, atualizar a política muito agressivamente pode levar ao aprendizado instável ou mudanças drásticas na política. Para lidar com isso, a PPO introduz uma restrição que limita até onde a atualização da política pode chegar. A restrição é reforçada com o uso da Divergência de Kullback-Leibler.\nPara entender como a Divergência de Kullback-Leibler funciona, imagine que temos duas distribuições de probabilidades: a distribuição do LLM original, e uma nova distribuição proposta de um LLM com RL atualizado. A Divergência de Kullback-Leibler mede a quantidade média de informação obtida quando usamos a política original a fim de codificar amostras da nova política proposta. Ao minimizar a Divergência de Kullback-Leibler entre as duas distribuições, a PPO garante que a política atualizada permaneça próxima à política original, prevenindo alterações drásticas que podem impactar negaticamente o processo de aprendizado.\nUma biblioteca que você pode usar para treinar os modelos de linguagem com o reforço de transformador de aprendizado, usando técnicas como PPO, é TRL (Aprendizado por Reforço de Transformador). Em"
}