{
    "source": "en_US",
    "target": "es_ES",
    "text": "In PPO, the goal is to find an improved policy for an agent by iteratively updating its parameters based on the rewards received from interacting with the environment. However, updating the policy too aggressively can lead to unstable learning or drastic policy changes. To address this, PPO introduces a constraint that limits the extent of policy updates. This constraint is enforced by using KL-Divergence.\nTo understand how KL-Divergence works, imagine we have two probability distributions: the distribution of the original LLM, and a new proposed distribution of an RL-updated LLM. KL-Divergence measures the average amount of information gained when we use the original policy to encode samples from the new proposed policy. By minimizing the KL-Divergence between the two distributions, PPO ensures that the updated policy stays close to the original policy, preventing drastic changes that may negatively impact the learning process.\nA library that you can use to train transformer language models with reinforcement learning, using techniques such as PPO, is TRL (Transformer Reinforcement Learning). In",
    "trans": "En PPO, el objetivo es encontrar una política mejorada para un agente, actualizando reiteradamente sus métricas, basándose en las recompensas recibidas de la interacción con el entorno. Sin embargo, actualizar la política demasiado agresivamente puede conducir a cambios drásticos de políticas o a un aprendizaje inestable. Para solucionarlo, PPO introduce un obstáculo que limita el alcance de las actualizaciones de políticas. Este obstáculo se ejecuta usando KL-Divergence.\nPara entender cómo funciona KL-Divergence, imagine que tenemos dos distribuciones de probabilidades: la distribución del LLM original y una nueva distribución propuesta de un LLM con RL actualizado. KL-Divergence mide la cantidad promedio de información obtenida cuando usamos la política original para codificar las muestras de la nueva política propuesta. Al minimizar la KL-Divergence entre las dos distribuciones, PPO garantiza que la política actualizada se mantenga cercana a la política original, lo que evita cambios drásticos que pueden afectar negativamente el proceso de aprendizaje.\nUna biblioteca que puede usar para entrenar modelos de lenguaje transformadores con aprendizaje de refuerzo, por medio de técnicas como PPO, es TRL (Aprendizaje de Refuerzo Transformador). En"
}