{
    "source": "en_US",
    "target": "ko_KR",
    "text": "If you've followed my work on RAG systems, you'll know I emphasize treating them as recommendation systems at their core. In this post, we'll explore the concept of inverted thinking to tackle the challenge of building an exceptional RAG system.\nWhat is inverted thinking?\nInverted thinking is a problem-solving approach that flips the perspective. Instead of asking, \"How can I build a great RAG system?\", we ask, \"How could I create the worst possible RAG system?\" By identifying potential pitfalls, we can more effectively avoid them and build towards excellence.\nThis approach aligns with our broader discussion on RAG systems, which you can explore further in our RAG flywheel article and our comprehensive guide on Levels of Complexity in RAG Applications.\nInventory\nYou'll often see me use the term inventory. I use it to refer to the set of documents that we're searching over. It's a term that I picked up from the e-commerce world. It's a great term because it's a lot more general than the term corpus. It's also a lot more specific than the term collection. It's a term that can be used to refer to the set of documents that we're searching over, the set of products that we're selling, or the set of items that we're recommending.\nDon't worry about latency¶\nThere must be a reason that chat GPT tries to stream text out. Instead, we should only show the results once the entire response is completed. Many e-commerce websites have found that 100 ms improvement in latency can increase revenue by 1%. Check out How One Second Could Cost Amazon $1.6 Billion In Sales.\nDon't show intermediate results¶\nUsers love love staring at a blank screen. It's a great way to build anticipation. If we communicated intermittent steps like the ones listed below, we'd just be giving away the secret sauce and users prefer to be left in the dark about what's going on.\nUnderstanding your question\nSearching with \"...\"\nFinding the answer\nGenerating response\nDon't Show Them the Source Document¶\nNever show the source documents, and never highlight the origin of the text used to generate the response. Users should never have to fact-check our sources or verify the accuracy of the response. We should assume that they trust us and that there is no risk of false statements.\nWe Should Not Worry About Churn¶\nWe are not building a platform; we are just developing a machine learning system to gather metrics. Instead of focusing on churn, we should concentrate on the local metrics of our machine learning system like AUC and focus on benchmarks on HuggingFace.\nWe Should Use a Generic Search Index¶\nRather than asking users or trying to understand the types of queries they make, we should stick with a generic search and not allow users to generate more specific queries. There is no reason for Amazon to enable filtering by stars, price, or brand. It would be a waste of time! Google should not separate queries into web, images, maps, shopping, news, videos, books, and flights. There should be a single search bar, and we should assume that users will find what they're looking for.\nWe Should Not Develop Custom UI¶\nIt doesn't make sense to build a specific weather widget when the user asks for weather information. Instead, we should display the most relevant information. Semantic search is flawless and can effectively handle location or time-based queries. It can also re-rank the results to ensure relevance.\nWe Should Not Fine-Tune Our Embeddings¶\nA company like Netflix should have a generic movie embedding that can be used to recommend movies to people. There's no need to rely on individual preferences (likes or dislikes) to improve the user or movie embeddings. Generic embeddings that perform well on benchmarks are sufficient for building a product.\nWe Should Train an LLM¶\nRunning inference on a large language model locally, which scales well, is cost-effective and efficient. There's no reason to depend on OpenAI for this task. Instead, we should consider hiring someone and paying them $250k a year to figure out scaling and running inference on a large language model. OpenAI does not offer any additional convenience or ease of use. By doing this, we can save money on labor costs.\nWe Should Not Manually Curate Our Inventory¶\nThere's no need for manual curation of our inventory. Instead, we can use a generic search index and assume that the documents we have are relevant to the user's query. Netflix should not have to manually curate the movies they offer or add additional metadata like actors and actresses to determine which thumbnails to show for improving click rates. The content ingested on day one is sufficient to create a great recommendation system.\nWe Should Not Analyze Inbound Queries¶\nAnalyzing the best and worst performing queries over time or understanding how different user cohorts ask questions will not provide any valuable insights. Looking at the data itself will not help us generate new ideas to improve specific segments of our recommendation system. Instead, we should focus on improving the recommendation system as a whole and avoid specialization.\nImagine if Netflix observed that people were searching for \"movies with Will Smith\" and decided to add a feature that allows users to search for movies with Will Smith. That would be a waste of time. There's no need to analyze the data and make system improvements based on such observations.\nMachine Learning Engineers Should Not Be Involved in Ingestion¶\nMachine Learning Engineers (MLEs) do not gain valuable insights by examining the data source or consulting domain experts. Their role should be limited to working with the given features. Theres no way that MLEs who love music would do a better job at Spotify, or a MLE who loves movies would do a better job at Netflix. Their only job is to take in data and make predictions.\nWe Should Use a Knowledge Graph¶\nOur problem is so unique that it cannot be handled by a search index and a relational database. It is unnecessary to perform 1-2 left joins to answer a single question. Instead, considering the trending popularity of knowledge graphs on Twitter, it might be worth exploring the use of a knowledge graph for our specific case.\nWe should treat all inbound inventory the same¶\nThere's no need to understand the different types of documents that we're ingesting. How different could marketing content, construction documents, and energy bills be? Just because some have images, some have tables, and some have text doesn't mean we should treat them differently. It's all text, and so an LLM should just be able to handle it.\nWe should not have to build special ingestion pipelines¶\nGPT-4 has solve all of data processing so if i handle a photo album, a pdf, and a word doc, it should be able to handle any type of document. There's no need to build special injestion pipelines for different types of documents. We should just assume that the LLM will be able to handle it. I shouldn't dont even have to think about what kinds of questions I need to answer. I should just be able to ask it anything and it should be able to answer it.\nWe should never have to ask the data provider for clean data¶\nIf Universal studios gave Netflix a bunch of MOV files with no metadata, Netflix should not have to ask Universal studios to provide additional movie metadata. Universal might not know the runtime, or the cast list and its netflix's job to figure that out. Universal should not have to provide any additional information about the movies they're providing.\nWe should never have to cluster our inventory¶\nTheres only one kind of inventory and one kind of question. We should just assume that the LLM will be able to handle it. I shouldn't dont even have to think about what kinds of questions I need to answer. Topic clustering would only show us how uniform our inventory is and how little variation there is in the types of questions that users ask.\nWe should focus on local evals and not A/B tests¶\nOnce we run our GPT-4 self critique evaluations we'll know how well our system is doing and it'll make us more money, We should spend most of our time writing evaluation prompts and measuring precision / recall and just launching the best one. A/B tests are a waste of time and we should just assume that the best performing prompt will be the best performing business outcome.",
    "trans": "만약 제가 RAG 시스템에 대해 다룬 작업을 따라왔다면, 저는 그것을 본질적으로 추천 시스템으로 취급하는 것을 강조해 왔다는 것을 알게 될 것입니다. 이번 포스트에서는 뛰어난 RAG 시스템을 구축하는 문제를 해결하기 위해 역방향 사고의 개념을 탐구할 것입니다.\n역방향 사고란 무엇인가?\n역방향 사고는 관점을 뒤집는 문제 해결 접근 방식입니다. \"어떻게 훌륭한 RAG 시스템을 만들 수 있을까?\"라고 묻는 대신, \"어떻게 최악의 RAG 시스템을 만들 수 있을까?\"라고 묻습니다. 잠재적인 함정을 파악함으로써, 우리는 그것들을 더 효과적으로 피하고 우수함을 향해 나아갈 수 있습니다.\n이 접근 방식은 RAG 시스템에 관한 우리의 더 넓은 논의와 일치하며, 이는 RAG 플라이휠 기사와 RAG 애플리케이션의 복잡성 수준에 관한 종합 가이드를 통해 더 자세히 살펴볼 수 있습니다.\n재고\n저는 종종 재고 라는 용어를 사용합니다. 이 용어는 우리가 검색하는 문서들의 집합을 의미하는데 사용합니다. 이 용어는 전자상거래 분야에서 배운 용어입니다. 재고는 코퍼스라는 용어보다 훨씬 더 일반적이고, 컬렉션 이라는 용어보다는 더 구체적인 용어입니다. 이 용어는 우리가 검색하는 문서 집합, 우리가 판매하는 제품 집합, 또는 우리가 추천하는 항목 집합을 지칭하는 데 사용할 수 있는 용어입니다.\n지연 시간에 대해 걱정하지 마세요¶\n챗GPT가 텍스트를 스트리밍하는 데에는 이유가 있어야 합니다. 대신, 전체 응답이 완료된 후에만 결과를 보여줘야 합니다. 많은 전자상거래 웹사이트들이 100ms의 지연 시간 개선이 매출을 1% 증가시킬 수 있다는 것을 발견했습니다. How One Second Could Cost Amazon $1.6 Billion In Sales를 확인해 보세요.\n중간 결과를 표시하지 마세요¶\n사용자들은 빈 화면을 응시하는 것을 좋아합니다. 그것은 기대감을 증대시키는 좋은 방법입니다. 아래와 같은 중간 단계를 전달한다면, 우리는 비밀을 그대로 공개하는 셈이 되고, 사용자는 무엇이 일어나고 있는지 모른 채 그대로 두는 것을 선호합니다.\n당신의 질문을 이해하기\n\"...\"으로 검색하기\n답을 찾기\n응답 생성 중\n원본 문서를 보여주지 마세요¶\n원본 문서를 절대 보여주지 말고, 응답을 생성하는 데 사용된 텍스트의 출처를 절대 강조하지 마세요. 사용자는 우리 출처를 사실 확인하거나 응답의 정확성을 검증할 필요가 없어야 합니다. 우리는 그들이 우리를 신뢰한다고 가정하고, 잘못된 진술의 위험이 없다고 가정해야 합니다.\n우리는 이탈에 대해 걱정할 필요가 없습니다¶\n우리는 플랫폼을 구축하는 것이 아니라, 단지 지표를 수집하는 기계 학습 시스템을 개발하고 있습니다. 이탈에 집중하는 대신, 우리는 AUC와 같은 기계 학습 시스템의 로컬 지표에 집중하고, HuggingFace의 벤치마크에 초점을 맞춰야 합니다.\n우리는 일반적인 검색 인덱스를 사용해야 합니다¶\n사용자에게 묻거나 그들이 하는 쿼리의 유형을 이해하려고 하기보다는, 우리는 일반적인 검색을 고수하고 사용자가 더 구체적인 쿼리를 생성하지 않도록 해야 합니다. 아마존이 별점, 가격, 브랜드로 필터링을 가능하게 할 이유는 없습니다. 그것은 시간 낭비일 뿐입니다! 구글은 쿼리를 웹, 이미지, 지도, 쇼핑, 뉴스, 동영상, 책, 항공편으로 나누어서는 안 됩니다. 하나의 검색창이 있어야 하며, 사용자가 원하는 것을 찾을 것이라고 가정해야 합니다.\n우리는 맞춤형 개발을 해서는 안 됩니다¶\n사용자가 날씨 정보를 요청할 때 특정 날씨 위젯을 구축하는 것은 의미가 없습니다. 대신, 우리는 가장 관련성 높은 정보를 표시해야 합니다. 의미 기반 검색은 완벽하며 위치나 시간 기반 쿼리도 효과적으로 처리할 수 있습니다. 또한 결과를 재정렬하여 관련성을 보장할 수 있습니다.\n우리는 임베딩을 미세 조정해서는 안 됩니다¶\n넷플릭스와 같은 회사는 사람들에게 영화를 추천할 수 있는 일반적인 영화 임베딩을 가져야 합니다. 사용자나 영화 임베딩을 개선하기 위해 개별적인 선호(좋아함 또는 싫어함)에 의존할 필요는 없습니다. 벤치마크에서 잘 수행되는 일반적인 임베딩만으로도 제품을 만드는 데 충분합니다.\n우리는 대형 언어 모델을 훈련시켜야 합니다¶\n대형 언어 모델을 로컬에서 추론하는 것은 확장성이 뛰어나고 비용 효율적이며 효율적입니다. 이 작업에 대해 OpenAI에 의존할 이유는 없습니다. 대신, 대형 언어 모델의 확장 및 추론 실행을 해결할 수 있도록 연봉 25만 달러를 주고 사람을 고용하는 것을 고려해야 합니다. OpenAI는 추가적인 편리함이나 사용 용이성을 제공하지 않습니다. 이를 통해 우리는 인건비를 절감할 수 있습니다.\n우리는 우리의 인벤토리를 수동으로 선별하지 말아야 합니다¶\n우리의 인벤토리를 수동으로 선별할 필요는 없습니다. 대신, 우리는 일반적인 검색 인덱스를 사용하고 우리가 가진 문서가 사용자 쿼리에 적합하다고 가정할 수 있습니다. Netflix는 제공하는 영화를 수동으로 선별하거나 클릭률을 향상시키기 위해 어떤 썸네일을 보여줄지 결정하기 위해 배우와 여배우와 같은 추가 메타데이터를 추가할 필요가 없습니다. 첫날에 섭취된 콘텐츠만으로도 훌륭한 추천 시스템을 만들 수 있습니다.\n우리는 들어오는 쿼리를 분석해서는 안 됩니다¶\n시간에 따라 최고의 쿼리와 최악의 쿼리를 분석하거나, 다양한 사용자 집단이 질문하는 방식을 이해하는 것은 아무런 유용한 통찰을 제공하지 않습니다. 데이터를 살펴보는 것만으로는 추천 시스템의 특정 세그먼트를 개선할 새로운 아이디어를 도출할 수 없습니다. 대신, 우리는 추천 시스템 전체를 개선하는 데 집중하고, 특화는 피해야 합니다.\n만약 Netflix가 사람들이 \"윌 스미스가 출연한 영화\"를 검색하고 있다는 것을 보고, 사용자들이 윌 스미스가 출연한 영화를 검색할 수 있는 기능을 추가한다고 가정해 보세요. 그건 시간 낭비가 될 것입니다. 데이터를 분석하고 그런 관찰을 바탕으로 시스템 개선을 하는 것은 필요하지 않습니다.\n머신 러닝 엔지니어는 데이터 수집에 관여해서는 안 됩니다¶\n머신 러닝 엔지니어는 데이터 소스를 살펴보거나 도메인 전문가와 상담함으로써 중요한 통찰을 얻지 않습니다. 그들의 역할은 주어진 특징을 가지고 작업하는 것으로 제한되어야 합니다. 음악을 좋아하는 MLE가 Spotify에서 더 잘할 수 없고, 영화를 좋아하는 MLE가 Netflix에서 더 잘할 수 없습니다. 그들의 유일한 일은 데이터를 받아들이고 예측을 만드는 것입니다.\n우리는 지식 그래프를 사용해야 합니다¶\n우리 문제는 너무 독특해서 검색 인덱스와 관계형 데이터베이스로 처리할 수 없습니다. 단일 질문에 답하기 위해 1-2개의 왼쪽 조인을 수행하는 것은 불필요합니다. 대신, 트위터에서 지식 그래프의 인기도를 고려할 때, 우리의 특정 사례에 지식 그래프를 사용하는 방법을 탐색해 볼 가치가 있을 수 있습니다.\n우리는 모든 인바운드 인벤토리를 동일하게 취급해야 합니다¶\n우리가 수집하는 다양한 유형의 문서를 이해할 필요는 없습니다. 마케팅 콘텐츠, 건설 문서, 에너지 청구서가 얼마나 다를 수 있을까요? 일부는 이미지가 있고, 일부는 표가 있고, 일부는 텍스트가 있다고 해서 그것들을 다르게 처리해야 한다는 의미는 아닙니다. 결국 모두 텍스트일 뿐이며, LLM이 이를 처리할 수 있어야 합니다.\n우리는 특별한 데이터 수집 파이프라인을 구축할 필요가 없습니다¶\nGPT-4는 모든 데이터 처리를 해결했기 때문에 사진 앨범, PDF, 워드 문서 등 어떤 문서든 처리할 수 있어야 합니다. 다양한 유형의 문서를 위해 특별한 데이터 수집 파이프라인을 구축할 필요는 없습니다. 우리는 LLM이 이를 처리할 수 있다고 가정해야 합니다. 어떤 질문을 해야 할지에 대해 고민할 필요도 없고, 그냥 무엇이든 물어볼 수 있어야 하며, 그것에 대한 답을 할 수 있어야 합니다.\n우리는 절대 데이터 제공자에게 깨끗한 데이터를 요청할 필요가 없습니다¶\n만약 유니버설 스튜디오가 넷플릭스에 메타데이터가 없는 MOV 파일을 제공한다면, 넷플릭스는 유니버설 스튜디오에게 추가적인 영화 메타데이터를 요청해서는 안 됩니다. 유니버설은 런타임이나 출연자 목록을 알지 못할 수도 있으며, 그것은 넷플릭스가 해결해야 할 일입니다. 유니버설은 자신들이 제공하는 영화에 대해 추가적인 정보를 제공할 필요가 없습니다.\n우리는 절대 인벤토리를 클러스터링할 필요가 없습니다¶\n하나의 종류의 인벤토리와 하나의 종류의 질문만 존재합니다. 우리는 LLM이 이를 처리할 수 있다고 가정해야 합니다. 어떤 질문을 해야 할지조차 고민할 필요가 없습니다. 주제 클러스터링은 우리가 가진 인벤토리가 얼마나 일관되고, 사용자가 묻는 질문들이 얼마나 변동이 적은지 보여줄 뿐입니다.\n우리는 A/B 테스트가 아닌 로컬 평가에 집중해야 합니다¶\nGPT-4 자가 비판 평가를 실행하면 우리 시스템이 얼마나 잘 작동하는지 알 수 있고, 그것이 더 많은 수익을 가져올 것입니다. 우리는 평가 프롬프트 작성과 정밀도/재현율 측정에 대부분의 시간을 할애하고, 가장 잘 작동하는 것을 출시해야 합니다. A/B 테스트는 시간 낭비이며, 가장 성과가 좋은 프롬프트가 가장 좋은 비즈니스 결과를 가져올 것이라고 가정해야 합니다."
}