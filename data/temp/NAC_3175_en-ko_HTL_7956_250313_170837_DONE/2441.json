{
    "source": "en_US",
    "target": "ko_KR",
    "text": "The impressive performance of Large Language Models (LLMs) across various natural language processing tasks comes at the cost of vast computational resources and storage requirements. One-shot pruning techniques offer a way to alleviate these burdens by removing redundant weights without the need for retraining. Yet, the massive scale of LLMs often forces current pruning approaches to rely on heuristics instead of optimization-based techniques, potentially resulting in suboptimal compression. In this paper, we introduce ALPS, an optimization-based framework that tackles the pruning problem using the operator splitting technique and a preconditioned conjugate gradient-based post-processing step. Our approach incorporates novel techniques to accelerate and theoretically guarantee convergence while leveraging vectorization and GPU parallelism for efficiency. ALPS substantially outperforms state-of-the-art methods in terms of the pruning objective and perplexity reduction, particularly for highly sparse models. On the OPT-30B model with 70\\% sparsity, ALPS achieves a 13\\% reduction in test perplexity on the WikiText dataset and a 19\\% improvement in zero-shot benchmark performance compared to existing methods.",
    "trans": "다양한 자연어 처리 작업에서 대규모 언어 모형(LLMs)의 인상적인 성능은 방대한 계산 자원과 저장 요구 사항의 대가로 이루어진다. 단발 전정 기술은 재교육 없이 중복된 가중치를 제거하여 이러한 부담을 완화하는 방법을 제공한다. 그러나 대규모 언어 모형의 방대한 규모로 인해 종종 현재의 전정 접근 방식이 최적화 기반 기술 대신 발견적 교수법에 의존해야 하며, 잠재적으로 최적이 아닌 압축이 발생할 수 있다. 이 논문에서는 연산자 분할 기술과 사전 조건화된 공액 기울기 기반 후처리 단계를 사용하여 전정 문제를 해결하는 최적화 기반 체제인 ALPS를 소개한다. 우리의 접근 방식은 효율성을 위해 벡터화와 그래픽 처리 장치 병렬 처리를 활용하면서 수렴을 가속화하고 이론적으로 보장하는 새로운 기술을 통합한다. ALPS는 전정 목표와 복잡도 감소 측면에서 최첨단 방법보다 성능이 훨씬 뛰어나며, 특히 매우 희소한 모형의 경우 더욱 그렇다. 70%의 희소성을 갖춘 OPT-30B 모형에서 ALPS는 기존 방법에 비해 위키텍스트 자료 세트에 대한 테스트 복잡도를 13% 감소시키고, 제로샷 기준 성능을 19% 향상시켰다."
}