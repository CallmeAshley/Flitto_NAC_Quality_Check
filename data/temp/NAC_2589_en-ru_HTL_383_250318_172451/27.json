{
    "source": "en_US",
    "target": "ru_RU",
    "text": "In PPO, the goal is to find an improved policy for an agent by iteratively updating its parameters based on the rewards received from interacting with the environment. However, updating the policy too aggressively can lead to unstable learning or drastic policy changes. To address this, PPO introduces a constraint that limits the extent of policy updates. This constraint is enforced by using KL-Divergence.\nTo understand how KL-Divergence works, imagine we have two probability distributions: the distribution of the original LLM, and a new proposed distribution of an RL-updated LLM. KL-Divergence measures the average amount of information gained when we use the original policy to encode samples from the new proposed policy. By minimizing the KL-Divergence between the two distributions, PPO ensures that the updated policy stays close to the original policy, preventing drastic changes that may negatively impact the learning process.\nA library that you can use to train transformer language models with reinforcement learning, using techniques such as PPO, is TRL (Transformer Reinforcement Learning). In",
    "trans": "В PPO цель состоит в том, чтобы найти улучшенную политику для агента, итеративно обновляя его параметры на основе вознаграждений, полученных от взаимодействия с окружающей средой. Однако слишком агрессивное обновление политики может привести к нестабильному обучению или резким изменениям политики. Для решения этой проблемы PPO вводит ограничение, которое ограничивает степень обновлений политики. Это ограничение обеспечивается с помощью KL-дивергенции.\nЧтобы понять, как работает KL-дивергенция, представьте себе, что у нас есть два распределения вероятностей: распределение оригинальной LLM и новое предложенное распределение RL-обновленной LLM. KL-дивергенция измеряет среднее количество информации, полученной при использовании оригинальной политики для кодирования образцов из новой предложенной политики. Минимизируя KL-дивергенцию между двумя распределениями, PPO обеспечивает то, что обновленная политика остается близкой к оригинальной политике, предотвращая резкие изменения, которые могут негативно повлиять на процесс обучения.\nБиблиотека, которую вы можете использовать для обучения языковых моделей трансформеров с использованием методов обучения с подкреплением, таких как PPO, называется TRL (Transformer Reinforcement Learning)."
}