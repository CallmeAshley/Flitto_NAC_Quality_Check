{
    "source": "en_US",
    "target": "uk_UA",
    "text": "In PPO, the goal is to find an improved policy for an agent by iteratively updating its parameters based on the rewards received from interacting with the environment. However, updating the policy too aggressively can lead to unstable learning or drastic policy changes. To address this, PPO introduces a constraint that limits the extent of policy updates. This constraint is enforced by using KL-Divergence.\nTo understand how KL-Divergence works, imagine we have two probability distributions: the distribution of the original LLM, and a new proposed distribution of an RL-updated LLM. KL-Divergence measures the average amount of information gained when we use the original policy to encode samples from the new proposed policy. By minimizing the KL-Divergence between the two distributions, PPO ensures that the updated policy stays close to the original policy, preventing drastic changes that may negatively impact the learning process.\nA library that you can use to train transformer language models with reinforcement learning, using techniques such as PPO, is TRL (Transformer Reinforcement Learning). In",
    "trans": "У PPO метою є знаходження покращеної політики для агента шляхом ітеративного оновлення його параметрів на основі винагород, отриманих від взаємодії з середовищем. Однак занадто агресивне оновлення політики може призвести до нестабільного навчання або різких змін у політиці. Для вирішення цієї проблеми PPO вводить обмеження, яке обмежує ступінь оновлень політики. Це обмеження забезпечується за допомогою KL-дивергенції.\nЩоб зрозуміти, як працює KL-дивергенція, уявіть, що у нас є два розподіли ймовірностей: розподіл оригінальної LLM та новий запропонований розподіл RL-оновленої LLM. KL-дивергенція вимірює середню кількість інформації, отриманої при використанні оригінальної політики для кодування вибірок з нової запропонованої політики. Мінімізуючи KL-дивергенцію між двома розподілами, PPO забезпечує те, що оновлена політика залишається близькою до оригінальної політики, запобігаючи різким змінам, які можуть негативно вплинути на процес навчання.\nБібліотека, яку ви можете використовувати для навчання трансформерних мовних моделей з підкріплювальним навчанням, використовуючи такі техніки, як PPO, - це TRL (Transformer Reinforcement Learning)."
}