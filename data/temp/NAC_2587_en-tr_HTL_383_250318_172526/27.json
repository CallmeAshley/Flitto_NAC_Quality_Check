{
    "source": "en_US",
    "target": "tr_TR",
    "text": "In PPO, the goal is to find an improved policy for an agent by iteratively updating its parameters based on the rewards received from interacting with the environment. However, updating the policy too aggressively can lead to unstable learning or drastic policy changes. To address this, PPO introduces a constraint that limits the extent of policy updates. This constraint is enforced by using KL-Divergence.\nTo understand how KL-Divergence works, imagine we have two probability distributions: the distribution of the original LLM, and a new proposed distribution of an RL-updated LLM. KL-Divergence measures the average amount of information gained when we use the original policy to encode samples from the new proposed policy. By minimizing the KL-Divergence between the two distributions, PPO ensures that the updated policy stays close to the original policy, preventing drastic changes that may negatively impact the learning process.\nA library that you can use to train transformer language models with reinforcement learning, using techniques such as PPO, is TRL (Transformer Reinforcement Learning). In",
    "trans": "PPO'da (Proximal Policy Optimization) amaç, bir ajanın çevreyle etkileşimden aldığı ödüllere dayanarak parametrelerini yinelemeli olarak güncelleyerek geliştirilmiş bir politika bulmaktır. Ancak, politikayı çok agresif bir şekilde güncellemek, öğrenmenin kararsız hale gelmesine veya politika değişikliklerinin aşırı olmasına yol açabilir. Bu sorunu ele almak için PPO, politika güncellemelerinin boyutunu sınırlayan bir kısıtlama getirir. Bu kısıtlama, KL-Divergence kullanılarak uygulanır.\nKL-Divergence'ın nasıl çalıştığını anlamak için iki olasılık dağılımı olduğunu hayal edin: orijinal dil modeli (LLM) dağılımı ve RL ile güncellenmiş LLM'nin yeni önerilen dağılımı. KL-Divergence, yeni önerilen politikadan örnekleri kodlamak için orijinal politikayı kullandığımızda kazanılan ortalama bilgi miktarını ölçer. PPO, iki dağılım arasındaki KL-Divergence'ı minimize ederek, güncellenmiş politikanın orijinal politikaya yakın kalmasını sağlar ve öğrenme sürecini olumsuz etkileyebilecek aşırı değişiklikleri önler.\nTransformer dil modellerini PPO gibi takviyeli öğrenme teknikleriyle eğitmek için kullanabileceğiniz bir kütüphane TRL'dir (Transformer Reinforcement Learning)."
}