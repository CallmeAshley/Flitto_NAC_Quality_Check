{
    "source": "en_US",
    "target": "pl_PL",
    "text": "In PPO, the goal is to find an improved policy for an agent by iteratively updating its parameters based on the rewards received from interacting with the environment. However, updating the policy too aggressively can lead to unstable learning or drastic policy changes. To address this, PPO introduces a constraint that limits the extent of policy updates. This constraint is enforced by using KL-Divergence.\nTo understand how KL-Divergence works, imagine we have two probability distributions: the distribution of the original LLM, and a new proposed distribution of an RL-updated LLM. KL-Divergence measures the average amount of information gained when we use the original policy to encode samples from the new proposed policy. By minimizing the KL-Divergence between the two distributions, PPO ensures that the updated policy stays close to the original policy, preventing drastic changes that may negatively impact the learning process.\nA library that you can use to train transformer language models with reinforcement learning, using techniques such as PPO, is TRL (Transformer Reinforcement Learning). In",
    "trans": "W algorytymie PPO celem jest znalezienie ulepszonej polityki dla agenta poprzez iteracyjne aktualizowanie jego parametrów na podstawie nagród otrzymywanych poprzez interakcje z otoczeniem. Jednakże, zbyt agresywne aktualizowanie polityki może prowadzić do niestabilnego uczenia lub drastycznych zmian w polityce. By to zmienić, PPO wprowadza ograniczenie które limituje efekt aktualizacji polityki. To ograniczenie działa dzięki Dywergencji Kullbacka-Leiblera.\nBy zrozumieć jak działa Dywergencja Kullbacka-Leiblera, wyobraźcie sobie, że mamy dwa rozkłady prawdopodobieństwa: rozkład oryginalnego dużego modelu językowego i nowy zaproponowany rozkład uczenia poprzez wzmacnianie dużego modelu językowego. Dywergencja Kullbacka-Leiblera mierzy średnią ilość informacji pozyskanej, gdy używamy oryginalnej polityki by zakodować próbi z nowej zaproponowanej politykki. Poprzez zminimalizowanie Dywergencji Kullbaca-Leiblera między dwoma rozkładami, PPO gwarantuje, że zaktualizowana polityka pozostanie blisko oryginalnej polityki, co zapobiega drastycznym zmianom, które mogą negatywnie wpłynąć na proces uczenia.\nBiblioteką, którą można wykorzystać do trenowania transformerów językowych za pomocą uczenia poprzez wzmacnianie, wykorzystując takie techniki jak PPO, jest TRL (Transformer Reinforcement Learning). W"
}