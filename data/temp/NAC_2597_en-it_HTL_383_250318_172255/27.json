{
    "source": "en_US",
    "target": "it_IT",
    "text": "In PPO, the goal is to find an improved policy for an agent by iteratively updating its parameters based on the rewards received from interacting with the environment. However, updating the policy too aggressively can lead to unstable learning or drastic policy changes. To address this, PPO introduces a constraint that limits the extent of policy updates. This constraint is enforced by using KL-Divergence.\nTo understand how KL-Divergence works, imagine we have two probability distributions: the distribution of the original LLM, and a new proposed distribution of an RL-updated LLM. KL-Divergence measures the average amount of information gained when we use the original policy to encode samples from the new proposed policy. By minimizing the KL-Divergence between the two distributions, PPO ensures that the updated policy stays close to the original policy, preventing drastic changes that may negatively impact the learning process.\nA library that you can use to train transformer language models with reinforcement learning, using techniques such as PPO, is TRL (Transformer Reinforcement Learning). In",
    "trans": "Nel PPO, l'obiettivo è trovare una politica migliorata per un agente aggiornando iterativamente i suoi parametri in base alle ricompense ricevute dall'interazione con l'ambiente. Tuttavia, aggiornare la politica in modo troppo aggressivo può portare a un apprendimento instabile o a drastici cambiamenti della politica. Per risolvere questo problema, PPO introduce un vincolo che limita l'estensione degli aggiornamenti della politica. Questo vincolo viene applicato utilizzando KL-Divergence.\nPer comprendere come funziona KL-Divergence, immaginiamo di avere due distribuzioni di probabilità: la distribuzione dell'LLM originale e una nuova distribuzione proposta di un RL LLM aggiornato. La KL-Divergence misura la quantità media di informazioni ottenute quando utilizziamo la policy originale per codificare campioni dalla nuova policy proposta. Riducendo al minimo la KL-Divergence tra le due distribuzioni, PPO assicura che la policy aggiornata rimanga vicina alla policy originale, impedendo drastici cambiamenti che potrebbero avere un impatto negativo sul processo di apprendimento.\nUna libreria che puoi utilizzare per addestrare modelli di linguaggio di trasformazione con apprendimento per rinforzo, utilizzando tecniche come PPO, è TRL (Transformer Reinforcement Learning). In"
}