{
    "source": "en_US",
    "target": "vi_VN",
    "text": "In PPO, the goal is to find an improved policy for an agent by iteratively updating its parameters based on the rewards received from interacting with the environment. However, updating the policy too aggressively can lead to unstable learning or drastic policy changes. To address this, PPO introduces a constraint that limits the extent of policy updates. This constraint is enforced by using KL-Divergence.\nTo understand how KL-Divergence works, imagine we have two probability distributions: the distribution of the original LLM, and a new proposed distribution of an RL-updated LLM. KL-Divergence measures the average amount of information gained when we use the original policy to encode samples from the new proposed policy. By minimizing the KL-Divergence between the two distributions, PPO ensures that the updated policy stays close to the original policy, preventing drastic changes that may negatively impact the learning process.\nA library that you can use to train transformer language models with reinforcement learning, using techniques such as PPO, is TRL (Transformer Reinforcement Learning). In",
    "trans": "Trong PPO, mục tiêu là tìm kiếm một chính sách cải thiện cho tác nhân bằng cách cập nhật tham số của nó dựa trên những phần thưởng nhận được qua các lần tương tác với môi trường. Tuy nhiên, việc cập nhật chính sách quá quyết liệt có thể dẫn đến việc học không ổn định hoặc thay đổi chính sách quá mức. Để giải quyết vấn đề này, PPO đưa ra một ràng buộc giới hạn mức độ cập nhật chính sách. Ràng buộc này được thực thi thông qua việc sử dụng KL-Divergence.\nĐể hiểu rõ hơn về cách thức hoạt động của KL-Divergence, hãy hình dung chúng ta có hai phân phối xác suất: phân phối của mô hình LLM gốc, và một phân phối mới được đề xuất của mô hình LLM đã được cập nhật qua RL. KL-Divergence đo lường lượng thông tin trung bình mà ta thu được khi sử dụng chính sách ban đầu để mã hóa các mẫu từ chính sách mới đề xuất. Bằng cách tối thiểu hóa KL-Divergence giữa hai phân phối này, PPO đảm bảo rằng chính sách đã được cập nhật sẽ gần với chính sách gốc, tránh được những thay đổi quá mức có thể tác động tiêu cực đến quá trình học.\nMột thư viện mà bạn có thể sử dụng để huấn luyện các mô hình ngôn ngữ máy học với học củng cố, dùng những kỹ thuật như PPO, là TRL (Transformer Reinforcement Learning). Trong"
}