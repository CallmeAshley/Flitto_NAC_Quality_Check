{
    "source": "en_US",
    "target": "uk_UA",
    "text": "Abstract:\nIn recent years, leveraging the powerful contextual understanding and extensive general knowledge of Large Language Models (LLMs) to enhance Temporal Knowledge Graphs (TKGs) Completion (TKGC) has emerged as a natural progression in research. However, the complexity of TKGs often prevents the generation of a Comprehensive and Concise Knowledge Sequence (CCKS) from historical knowledge through manually defined rules, which is essential for LLMs to infer future knowledge. To address this, we propose the Knowledge Attention via Radial Basis Functions (KA-RBF) model. Firstly, KA-RBF generates Original Knowledge Sequences (OKS) based on a simple manually defined rule. Building on this, KA-RBF uses the knowledge in the original training set along with the corresponding OKS of the head and tail entities as training samples, optimizing to enhance the attention on similar knowledge pairs. Next, KA-RBF scores the knowledge at each timestamp in the OKS of the target prediction sample to filter knowledge and generate the CCKS. Finally, we conducted three different control experiments and validated the effectiveness of KA-RBF across four different datasets. Additionally, we performed multi-faceted data analysis experiments on the existing four benchmark datasets, further reinforcing the feasibility of KA-RBF while also highlighting future research directions for TKGC.\nLast Modified: Wed, 09 Oct 2024 08:04:22 GMT\nCreated on: Wed, 09 Oct 2024 08:04:22 GMT",
    "trans": "Анотація:\nОстанніми роками використання потужного контекстуального розуміння та широких загальних знань великих мовних моделей (LLMs) для покращення заповнення часових онтологічних графів (TKGs) стало природним напрямком досліджень. Однак складність TKG часто ускладнює створення Комплексної та Стиснутої Послідовності Знань (CCKS) з історичних даних за допомогою вручну визначених правил, що є ключовим для LLM у прогнозуванні майбутніх знань.  Щоб вирішити цю проблему, ми пропонуємо модель Knowledge Attention via Radial Basis Functions (KA-RBF). По-перше, KA-RBF генерує Оригінальні Послідовності Знань (OKS) на основі простого вручну визначеного правила. Далі KA-RBF використовує знання з початкового навчального набору разом із відповідними OKS для головних і кінцевих сутностей як навчальні вибірки, оптимізуючи увагу до схожих пар знань.  Потім KA-RBF оцінює знання на кожній часовій позначці в OKS для цільового прогнозованого зразка, фільтруючи знання та створюючи CCKS. Нарешті, ми провели три різні контрольні експерименти та підтвердили ефективність KA-RBF на чотирьох різних наборах даних. Крім того, ми виконали багатовимірний аналіз даних на існуючих чотирьох еталонних наборах даних, що додатково підтвердило доцільність KA-RBF і підкреслило майбутні напрями досліджень у сфері TKGC.\nОстаннє змінення: Ср, 09 жовтня 2024, 08:04:22 GMT\nСтворено: Ср, 09 жовтня 2024, 08:04:22 GMT"
}