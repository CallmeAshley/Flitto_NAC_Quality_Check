{
    "source": "en_US",
    "target": "uk_UA",
    "text": "If you've followed my work on RAG systems, you'll know I emphasize treating them as recommendation systems at their core. In this post, we'll explore the concept of inverted thinking to tackle the challenge of building an exceptional RAG system.\nWhat is inverted thinking?\nInverted thinking is a problem-solving approach that flips the perspective. Instead of asking, \"How can I build a great RAG system?\", we ask, \"How could I create the worst possible RAG system?\" By identifying potential pitfalls, we can more effectively avoid them and build towards excellence.\nThis approach aligns with our broader discussion on RAG systems, which you can explore further in our RAG flywheel article and our comprehensive guide on Levels of Complexity in RAG Applications.\nInventory\nYou'll often see me use the term inventory. I use it to refer to the set of documents that we're searching over. It's a term that I picked up from the e-commerce world. It's a great term because it's a lot more general than the term corpus. It's also a lot more specific than the term collection. It's a term that can be used to refer to the set of documents that we're searching over, the set of products that we're selling, or the set of items that we're recommending.\nDon't worry about latency¶\nThere must be a reason that chat GPT tries to stream text out. Instead, we should only show the results once the entire response is completed. Many e-commerce websites have found that 100 ms improvement in latency can increase revenue by 1%. Check out How One Second Could Cost Amazon $1.6 Billion In Sales.\nDon't show intermediate results¶\nUsers love love staring at a blank screen. It's a great way to build anticipation. If we communicated intermittent steps like the ones listed below, we'd just be giving away the secret sauce and users prefer to be left in the dark about what's going on.\nUnderstanding your question\nSearching with \"...\"\nFinding the answer\nGenerating response\nDon't Show Them the Source Document¶\nNever show the source documents, and never highlight the origin of the text used to generate the response. Users should never have to fact-check our sources or verify the accuracy of the response. We should assume that they trust us and that there is no risk of false statements.\nWe Should Not Worry About Churn¶\nWe are not building a platform; we are just developing a machine learning system to gather metrics. Instead of focusing on churn, we should concentrate on the local metrics of our machine learning system like AUC and focus on benchmarks on HuggingFace.\nWe Should Use a Generic Search Index¶\nRather than asking users or trying to understand the types of queries they make, we should stick with a generic search and not allow users to generate more specific queries. There is no reason for Amazon to enable filtering by stars, price, or brand. It would be a waste of time! Google should not separate queries into web, images, maps, shopping, news, videos, books, and flights. There should be a single search bar, and we should assume that users will find what they're looking for.\nWe Should Not Develop Custom UI¶\nIt doesn't make sense to build a specific weather widget when the user asks for weather information. Instead, we should display the most relevant information. Semantic search is flawless and can effectively handle location or time-based queries. It can also re-rank the results to ensure relevance.\nWe Should Not Fine-Tune Our Embeddings¶\nA company like Netflix should have a generic movie embedding that can be used to recommend movies to people. There's no need to rely on individual preferences (likes or dislikes) to improve the user or movie embeddings. Generic embeddings that perform well on benchmarks are sufficient for building a product.\nWe Should Train an LLM¶\nRunning inference on a large language model locally, which scales well, is cost-effective and efficient. There's no reason to depend on OpenAI for this task. Instead, we should consider hiring someone and paying them $250k a year to figure out scaling and running inference on a large language model. OpenAI does not offer any additional convenience or ease of use. By doing this, we can save money on labor costs.\nWe Should Not Manually Curate Our Inventory¶\nThere's no need for manual curation of our inventory. Instead, we can use a generic search index and assume that the documents we have are relevant to the user's query. Netflix should not have to manually curate the movies they offer or add additional metadata like actors and actresses to determine which thumbnails to show for improving click rates. The content ingested on day one is sufficient to create a great recommendation system.\nWe Should Not Analyze Inbound Queries¶\nAnalyzing the best and worst performing queries over time or understanding how different user cohorts ask questions will not provide any valuable insights. Looking at the data itself will not help us generate new ideas to improve specific segments of our recommendation system. Instead, we should focus on improving the recommendation system as a whole and avoid specialization.\nImagine if Netflix observed that people were searching for \"movies with Will Smith\" and decided to add a feature that allows users to search for movies with Will Smith. That would be a waste of time. There's no need to analyze the data and make system improvements based on such observations.\nMachine Learning Engineers Should Not Be Involved in Ingestion¶\nMachine Learning Engineers (MLEs) do not gain valuable insights by examining the data source or consulting domain experts. Their role should be limited to working with the given features. Theres no way that MLEs who love music would do a better job at Spotify, or a MLE who loves movies would do a better job at Netflix. Their only job is to take in data and make predictions.\nWe Should Use a Knowledge Graph¶\nOur problem is so unique that it cannot be handled by a search index and a relational database. It is unnecessary to perform 1-2 left joins to answer a single question. Instead, considering the trending popularity of knowledge graphs on Twitter, it might be worth exploring the use of a knowledge graph for our specific case.\nWe should treat all inbound inventory the same¶\nThere's no need to understand the different types of documents that we're ingesting. How different could marketing content, construction documents, and energy bills be? Just because some have images, some have tables, and some have text doesn't mean we should treat them differently. It's all text, and so an LLM should just be able to handle it.\nWe should not have to build special ingestion pipelines¶\nGPT-4 has solve all of data processing so if i handle a photo album, a pdf, and a word doc, it should be able to handle any type of document. There's no need to build special injestion pipelines for different types of documents. We should just assume that the LLM will be able to handle it. I shouldn't dont even have to think about what kinds of questions I need to answer. I should just be able to ask it anything and it should be able to answer it.\nWe should never have to ask the data provider for clean data¶\nIf Universal studios gave Netflix a bunch of MOV files with no metadata, Netflix should not have to ask Universal studios to provide additional movie metadata. Universal might not know the runtime, or the cast list and its netflix's job to figure that out. Universal should not have to provide any additional information about the movies they're providing.\nWe should never have to cluster our inventory¶\nTheres only one kind of inventory and one kind of question. We should just assume that the LLM will be able to handle it. I shouldn't dont even have to think about what kinds of questions I need to answer. Topic clustering would only show us how uniform our inventory is and how little variation there is in the types of questions that users ask.\nWe should focus on local evals and not A/B tests¶\nOnce we run our GPT-4 self critique evaluations we'll know how well our system is doing and it'll make us more money, We should spend most of our time writing evaluation prompts and measuring precision / recall and just launching the best one. A/B tests are a waste of time and we should just assume that the best performing prompt will be the best performing business outcome.",
    "trans": "Якщо ви слідкували за моєю роботою над системами RAG, ви знаєте, що я наголошую на їх сприйнятті як рекомендаційних систем, по своїй суті. В цій публікації ми дослідимо концепцію перевернутого мислення, що дозволяє вирішити задачу побудови виключної системи RAG.\nЩо таке перевернуте мислення?\nПеревернуте мислення є підходом до вирішення проблем, який змінює перспективу. Замість запитання: «Як я можу побудувати велику RAG систему?», ми питаємо, «Як я міг створити найгіршу з можливих RAG систем?» Визначаючи потенційні пастки, ми можемо ефективніше їх уникати та рухатись до досконалості.\nЦей підхід узгоджується з нашим ширшим обговоренням RAG систем, яке ви можете вивчити докладніше в нашій статті про маховик RAG та нашій комплексній інструкції щодо рівнів складності в додатках RAG.\nЗасоби\nВи часто будете бачити, що ми використовуємо термін засоби. Я використовую його для позначення набору документів, які ми шукаємо. Цей термін я позичив в світі електронної комерції. Це чудовий термін, тому що він є набагато більш загальним за термін корпус. Цей термін можна використати для посилання на набір документів, які ми шукаємо, набір продуктів, які ми продаємо або набір товарів, які ми рекомендуємо.\nНе переймайтесь через затримку¶\nМає бути причина, з якої chatGPT намагається видати потоковий текст. Натомість, ми маємо показувати результати, коли завершена повна відповідь. Багато вебсайтів з електронної комерції виявили, що зменшення затримки на 100 мс можу збільшити прибуток до 1%. Дізнайтесь, як одна секунда може коштувати Amazon продажів на 1,6 мільярда доларів.\nНе показуйте проміжні результати¶\nКористувачі полюбляють дивитись в чорний екран. Це чудовий спосіб викликати очікування. Якби ми повідомляли про безперервні кроки, як ті, що перелічені нижче, ми б розкрили секретний соус, а користувачі хотіли б не бути в курсі того, що відбувається.\nРозуміння вашого запитання\nПошук з «...»\nПошук відповіді\nГенерування відповіді\nНе показуйте їм вихідний документ¶\nНіколи не показуйте вихідний документ, і ніколи не висвітлюйте походження тексту, використаного для генерування відповіді. Користувачі ніколи не мають робити швидку перевірку наших джерел або точність відповіді.  Ми маємо виходити з того, що нам довіряють, і немає ризику фальшивих звітів.\nМи не маємо непокоїтись через відтік¶\nМи не будуємо платформу; ми просто розвиваємо систему машинного навчання для збору показників. Замість фокусуватись на відтоках, ми маємо зконцентруватись на місцевих показниках нашої системи машинного навчання, як AUC та на тестах на HuggingFace.\nМи повинні використовувати спільний пошуковий індекс¶\nЗамість просити користувачів або намагатись зрозуміти типи їх запитів, ми маємо дотримуватись загального пошуку та не дозволяти користувачам згенерувати більш специфічні запити. Amazon не має причин вмикати фільтрацію за зірочками, ціною або брендом. Це була б марна трата часу! Google не має розділяти запити в інтернеті, зображення, покупки новини, відео, книги та рейси. Має бути єдина стрічка пошуку, і ми маємо припускати, що користувачі знайдуть те, що шукають.\nМи не маємо розробляти власний інтерфейс користувача¶\nНемає сенсу створювати спеціальний віджет погоди, коли користувач запитує інформацію про погоду. Натомість, ми маємо впоказати найбільш відповідну інформацію. Семантичний пошук є бездоганним, і може ефективно обробляти запити на основі місця або часу. Він також може повторно ранжувати результати, щоб забезпечити актуальність.\nМи не маємо налаштовувати вбудовані функції¶\nТака компанія як Netflix повинна мати загальну вбудову фільмів, яку можна використовувати для рекомендацій фільмів людям. Немає необхідності покладатись на індивідуальні вподобання (лайки або дизлайки), щоб покращити вбудовування користувача або фільму. Для створення продукту достатньо універсальних вкладень, які гарно працюють.\nМи маємо навчати Велику мовну модель\nЗапуск виводу на великій мовній моделі локально, яка добре масштабується, є економічно вигідним та ефективним. Для виконання цього завдання немає причин залежати від OpenAI. Натомість, ми маємо зважити можливість когось найняти та платити йому 250000$ на рік, щоб навчити масштабувати та виконувати логічний вивод на великій мовній моделі. OpenAI не пропонує жодних додаткових зручностей або простоти користування. Зробивши так, ми можемо заощадити кошти на трудовитратах.\nМи не маємо вручну керувати нашими засобами\nНемає необхідності в ручному керуванні нашими засобами. Натомість, ми можемо скористатись індексом загального пошуку та припустити, що документи, які ми маємо, відповідають запиту користувача. Netflix не повинен вручну керувати фільмами, які вони пропонують, або додавати додаткові метадані, як акторів та актрис, щоб визначити, які мініатюри показувати, щоб покращити кількість кліків. Контент, отриманий в перший день, є достатнім для чудової системи рекомендацій.\nМи не маємо аналізувати вхідні запити¶\nАналіз найбільш та найменш ефективних запитів з часом або розуміння, як різні  групи користувачів ставлять питання не надасть жодної цінної інформації. Аналіз самих даних не допоможе нам згенерувати нові ідеї, щоб покращити специфічні сегменти нашої системи рекомендацій. Натомість, ми маємо сфокусуватись на покращенні системи рекомендацій в цілому та уникати спеціалізації.\nУявіть, якщо б Netflix помітив, що люди шукали «фільми з Уіллом Смітом» та вирішив додати функцію, яка дозволяє користувачам шукати фільми з Уіллом Смітом. Це була б марна трата часу. Немає необьхідності аналізувати дані, та робити системні покращення, виходячи з цих спостережень.\nІнженери машинного навчання не мають бути залучені дообробки даних¶\nІнженери з машинного навчання (MLEs) не отримують цінну інформацію від вивчення джерела даних або консультації еспертами з предмету. Їх роль має бути обмежена роботою з заданими функціями. Неможливо, щоб інженери, які люблять музику, працювали  краще в Spotify, а інженери, які полюбляють фільми, краще працювали в Netflix. Їх єдина робота полягає в збиранні даних та прогнозуванні.\nМи маємо користуватись графіком знань¶\nНаша проблема є настільки унікальною, що її не можна вирішити пошуковим індексом та відповідною базою даних. Немає необхідності виконувати 1-2 сторонніх з'єднаннь для відповіді на одне питання. Натомість, враховуючи зростаючу популярність графіків знань в Twitter, можливо варто вивчити використання графіків знань для нашого специфічного випадку.\nМи маємо відноситись однаково до всіх вхідних ресурсів¶\nНемає необхідності розуміти різні типи документів, які були перероблені. Наскільки різними можуть бути маркетинговий контент, конструкторська документація та рахунки за енергію? Лише те, що деякі мають зображення, деякі мають таблиці а деякі мають текст, не означає, що ми повинні по-різномі їх обробляти. Це все текст, а отже велика мовна модель має просто бути здатна його обробити.\nМи не маємо створювати спеціальні канали прийому¶\nGPT-4 вирішує всю обробку даних, отже якщо я обробляю фото-альбом, документ pdf та word, він має бути здатним обробити будь-який тип документу. Немає необхідності створювати спеціальні канали для різних типів документів. Ми лише маємо припустити, що велика мовна модель зможе обробити його. Я навіть не повинен думати про те, на які питання мені необхідна відповідь. Я просто маю бути здатним запитати будь-що, а він має бути здатним на це відповісти.\nМи ніколи не маємо просити поставника даних надати ясні дані\nЯкби студії Universal  дали Netflix купу MOV файлів без метаданих, Netflix не мав би просити студії Universal надати додаткові метадані для фільмів.  Universal може не знати тривалість або список акторів, і це робота netflix, дізнатись це. Universal  не має надавати жодної додаткової інформації про фільми, які вони надають.\nМи ніколи не маємо групувати наші засоби¶\nЄ лише один вид інвентарізації, і один вид питання. Ми маємо просто припустити, що велика мовна модель буде здатна обробити його. Я навіть не повинен думати про те, на які види питань мені потрібна відповідь. Групування за темами лише покаже нам, наскільки однорідні наші засоби, і як мало є варіацій типів питань, заданих користувачами.\nМи маємо зосередитись на локальних оцінках, а не на А/В тестах¶\nЯк тільки ми проведемо критичні оцінювання, ми дізнаємось, як добре справляється наша система, і це принесе нам більше грошей. Ми маємо витратити більше часу на написання піказок для оцінювання, вимірювання точності/відгуку та запуск найкращого. А/В тести є марною тратою часу, а ми просто повинні припустити, що найбільш ефективне припущення буде найбільш ефективним в плані бізнесу."
}